\documentclass[12pt]{article}

% Paquetes comunes
\usepackage[utf8]{inputenc}     % Codificación
\usepackage[T1]{fontenc}        % Fuente
\usepackage[english]{babel}     % Idioma
\usepackage{amsmath, amssymb}   % Matemáticas
\usepackage{graphicx}           % Imágenes
\usepackage{float}              % Posición de figuras
%\usepackage{cite}               % Citas
\usepackage{hyperref}           % Enlaces
\usepackage{geometry}           % Márgenes
\geometry{a4paper, margin=2.5cm}
\usepackage{color}              % Colores 
\usepackage{siunitx}            % SI units  

\usepackage{csquotes}
%\usepackage[style=numeric,sorting=none]{biblatex}

\usepackage[backend=biber,style=numeric,sorting=none]{biblatex}
\addbibresource{refs.bib}  

\usepackage[inline]{enumitem} % Paquete necesario para listas en línea
\setlist[enumerate,1]{label=(\roman*)} % Estilo de la etiqueta: (i), (ii), 

\usepackage{booktabs} % Para tablas
\usepackage{tabularx}
\usepackage{threeparttable}
\usepackage{multirow}
\usepackage{makecell}

\usepackage{pgfplots}
\usetikzlibrary{matrix}
\pgfplotsset{compat=1.18}


\title{Detection of Vascular Pathologies in Non-Contrast Magnetic Resonance Images for Early Screening} %of Cerebrovascular Diseases}
\author{Patricia García-Berlanga, Juan F. Zapata-Pérez, \\ 
        Pablo Hernández Cerdán, \\ 
        Juan de la Cruz Mártinez Cabeza de Vaca-Alajarín}
\date{\today}

\begin{document}

\maketitle

% Original abstract kept commented for context (minor edits allowed in comments)
%\abstract{
%Cerebrovascular diseases (CVD) are a leading cause of death and disability worldwide. Early detection and treatment are crucial to prevent severe outcomes. Magnetic Resonance Imaging (MRI) is a powerful tool for diagnosing CVD, but the use of contrast agents can pose risks to patients. This study aims to develop a method for detecting vascular pathologies in non-contrast MRI images, enabling early screening of CVD. We propose a deep learning-based approach that leverages convolutional neural networks (CNNs) to analyze non-contrast MRI scans and identify vascular abnormalities. The model is trained on a dataset of annotated MRI images, and its performance is evaluated using metrics such as accuracy, sensitivity, and specificity. Preliminary results indicate that the proposed method can effectively detect vascular pathologies, with potential applications in clinical settings for early CVD screening. Further research will focus on refining the model and validating its performance on larger datasets.
%
%Accurate segmentation of ischemic stroke lesions in magnetic resonance imaging (MRI) is essential for early diagnosis and treatment planning. This study evaluates and compares the performance of three deep learning architectures—U-Net, nnU-Net, and YOLOv8—for the automatic segmentation of ischemic stroke lesions using FLAIR MRI sequences without contrast agents. The models were trained and tested on the ISLES 2022 public dataset, containing 250 patient cases. U-Net was manually configured as a 3D baseline model, nnU-Net was applied as a self-configuring 3D full-resolution mode, and YOLOv8 was adapted from object detection to 2D medical image segmentation. Performance was assessed using the Dice coefficient and Intersection over Union (IoU) metrics. Results show that nnU-Net achieved the best overall accuracy, with a mean Dice score of 0.483 ± 0.284, outperforming both U-Net and YOLOv8. YOLOv8 demonstrated the fastest inference and competitive segmentation accuracy, indicating potential for real-time clinical use. U-Net exhibited limited generalization across heterogeneous lesions. In conclusion, nnU-Net remains the most reliable and adaptive solution for MRI stroke lesion segmentation, whereas YOLOv8 offers a promising lightweight alternative for time-critical applications. These findings highlight that automated segmentation of ischemic lesions from non-contrast FLAIR MRI is feasible and may support faster, more objective, and clinically scalable stroke assessment.
%
%resumen de lo anterior. Abstract, adaptado a este trabajo:
%
%Cerebrovascular diseases (CVD) are a leading cause of death and disability worldwide, making early detection and treatment crucial to prevent severe outcomes. This study addresses the clinical need for non-contrast MRI-based detection methods by developing a deep learning approach to identify vascular pathologies and segment ischemic stroke lesions without contrast agents. We evaluated and compared three convolutional neural network architectures—U-Net, nnU-Net, and YOLOv8—for automatic segmentation of ischemic stroke lesions using FLAIR MRI sequences. The models were trained and tested on the ISLES 2022 public dataset containing 250 patient cases. Performance was assessed using Dice coefficient and Intersection over Union (IoU) metrics. Results demonstrate that nnU-Net achieved the best overall accuracy with a mean Dice score of 0.483 ± 0.284, outperforming both U-Net and YOLOv8 in reliability and generalization across heterogeneous lesions. Conversely, YOLOv8 demonstrated the fastest inference speed and competitive segmentation accuracy, positioning it as a promising lightweight alternative for time-critical clinical applications. These findings confirm that automated segmentation of ischemic lesions from non-contrast FLAIR MRI is feasible and clinically viable, supporting faster, more objective, and scalable stroke assessment in clinical settings. Further research will focus on validating these models on larger datasets and refining their performance for enhanced early CVD screening capabilities.
%
%}

\abstract{
Early and non-invasive detection of cerebrovascular pathologies is essential to improve patient triage and outcomes. Contrast-free FLAIR MRI sequences are widely available in clinical practice but pose challenges for automated analysis due to variable lesion appearance, low contrast, and inter-scanner variability. In this study, we evaluate and compare three deep learning strategies for ischemic lesion segmentation in non-contrast FLAIR images: a classical 3D U-Net, the self-configuring nnU-Net framework, and an adapted 2D YOLOv8 segmentation variant. All methods were trained and tested using the ISLES 2022 dataset (N = 250 cases) under matched preprocessing and evaluation protocols to ensure a fair comparison.
%
We report quantitative performance using Dice similarity coefficient and Intersection over Union (IoU), and analyze trade-offs between accuracy, robustness and inference efficiency. The nnU-Net achieved the highest average overlap (Dice $= 0.483 \pm 0.284$), demonstrating superior robustness across heterogeneous lesion presentations. The YOLOv8-based model provided competitive volumetric performance (Dice $\approx 0.476$) while substantially reducing inference time, suggesting suitability for time-constrained clinical workflows. The classical 3D U-Net served as a manual baseline and exhibited lower mean overlap (Dice $\approx 0.294$), indicating sensitivity to parameter tuning and dataset variability.
%
Beyond numerical metrics, we qualitatively examine typical failure modes (small peripheral lesions, diffuse low-contrast areas and spatial mismatches between masks and images) and discuss preprocessing steps that mitigate them (spatial resampling, intensity normalization, and patch-based training). Finally, we outline practical recommendations for deploying automated segmentation in clinical settings: favor self-configuring frameworks like nnU-Net when compute resources permit, consider lightweight 2D models such as YOLOv8 for fast screening, and ensure rigorous cross-site validation before clinical use.
}

%\tableofcontents
%\newpage


\section{Introduction} 
\label{sec:introduction}

Ischemic stroke remains a leading cause of death and long-term disability worldwide, imposing substantial clinical and economic burdens on healthcare systems. Rapid and accurate identification of ischemic lesions is critical to guide acute management and rehabilitation planning. In routine clinical practice, Fluid-Attenuated Inversion Recovery (FLAIR) MRI is frequently acquired because it enhances the conspicuity of edema and chronic infarcts without the need for intravenous contrast. Its wide availability across scanners and centers makes FLAIR an attractive input for automated analysis pipelines.

However, FLAIR images pose considerable challenges for automated interpretation. Lesions can be small and highly heterogeneous in intensity and shape, and their appearance is often confounded by age-related white matter hyperintensities, motion artifacts, and scanner-dependent variations. These factors limit the effectiveness of traditional intensity-based and handcrafted-feature methods, which are sensitive to imaging artifacts and anatomical variability. In addition, the relative scarcity of large, well-annotated ischemic stroke datasets—particularly for FLAIR sequences—constrains the generalization of classical pipelines.

Deep learning (DL), especially convolutional neural networks (CNNs), has substantially advanced medical image segmentation by learning hierarchical features directly from data, reducing the need for manual feature engineering. Architectures such as U-Net and its 3D variants are widely used baselines, while frameworks like nnU-Net demonstrate that automated protocol selection and self-configuration frequently outperform manually tuned pipelines on heterogeneous medical datasets~\cite{Isensee2021}. Concurrently, lightweight and real-time capable models originally developed for natural images (for example, recent YOLO variants) have been adapted for segmentation tasks, offering favourable inference speed that can be important in time-sensitive clinical workflows.

Despite these advances, controlled comparisons across fundamentally different paradigms—manual 3D models, self-configuring medical frameworks, and adapted 2D real-time models—are limited for ischemic stroke segmentation on non-contrast FLAIR data. Understanding trade-offs between accuracy, robustness and inference efficiency in clinically realistic settings requires systematic evaluation under matched preprocessing and evaluation protocols.

In this work, we present a reproducible and controlled comparison of three representative strategies for ischemic lesion segmentation on non-contrast FLAIR MRI: (i) a manually configured 3D U-Net, (ii) the self-adapting nnU-Net framework (3D full-resolution), and (iii) a YOLOv8-based segmentation variant adapted to slice-wise FLAIR inputs. All models are trained and evaluated on the ISLES 2022 public dataset (N = 250) using identical preprocessing steps (spatial resampling, intensity normalization and mask alignment), matched data splits, and common evaluation metrics to minimize sources of bias.

Beyond aggregate Dice and IoU scores, we analyze robustness across lesion sizes and types, characterize common failure modes (for example, missed peripheral lesions and over-segmentation near white-matter hyperintensities), and measure inference efficiency to inform practical deployment trade-offs. Our main contributions are: (i) a controlled empirical comparison of three complementary segmentation paradigms on FLAIR stroke data; (ii) a detailed analysis of preprocessing and training choices that materially affect performance and generalization; and (iii) pragmatic recommendations for integrating automated segmentation into clinical or research workflows, balancing accuracy, robustness and computational cost.

The remainder of the manuscript is organized as follows. Section~\ref{sec:relatedwork} reviews related work; Section~\ref{sec:methodology} describes dataset preparation and model implementations; Section~\ref{sec:results} presents quantitative and qualitative results; Section~\ref{sec:discussion} discusses implications, limitations and future directions; and Section~\ref{sec:conclusion} concludes.

\section{Related Work} 
\label{sec:relatedwork}

% Original Related Work preserved below for reference (commented):
% Medical image segmentation has been a focal point of research for several decades, with significant advancements driven by both classical image processing techniques and the more recent adoption of deep learning methodologies. This section provides an overview of the key developments in this field, highlighting the evolution from traditional methods to state-of-the-art deep learning architectures.
%
%\subsection{Scientific and Technical Context} \label{subsec:context}
%
%While the clinical motivation for ischemic stroke lesion segmentation has been outlined in Section 1, it is equally important to situate this problem within the broader technical landscape of medical image analysis. Stroke lesion segmentation is among the most difficult challenges in brain imaging because of several specific characteristics: (i) lesions exhibit high inter-patient variability in size, shape, and location; (ii) tissue contrast in relevant MRI sequences such as FLAIR is often low, making lesions difficult to distinguish from surrounding white matter; (iii) infarcts evolve over time, so the appearance of acute, subacute, and chronic lesions differs substantially; and (iv) motion artifacts and scanner-dependent variations further complicate analysis. These aspects make the problem more complex than other established tasks such as brain tumor segmentation or multiple sclerosis lesion detection.
%
%A major driver of technical progress has been the creation of benchmark challenges, in particular the ISLES series (Ischemic Stroke Lesion Segmentation) organized within the MICCAI community. These challenges have provided publicly available datasets, standardized evaluation metrics, and competitive leaderboards, enabling direct comparison across methods. Through ISLES, researchers have repeatedly confirmed that conventional intensity-based methods struggle, while data-driven and deep learning techniques consistently achieve superior performance. 
%
%\subsection{Classification of Existing Approaches} \label{subsec:classification}
%
%Segmentation strategies for stroke imaging can be categorized into three major groups: traditional image processing methods, classical machine learning, and modern deep learning-based approaches.
%
%Traditional image processing methods were the first to be applied to medical image segmentation. Thresholding techniques assign pixels to foreground or background based on intensity values, but they fail when tissue contrast is low. Region growing methods start from a seed point and iteratively aggregate neighboring pixels with similar intensity; however, they are sensitive to noise and initialization. Clustering approaches, such as k-means, group voxels according to statistical similarity, but lack spatial constraints and often oversegment complex structures. For instance, K-means has been applied in stroke lesion segmentation to identify hyperintense regions in brain images \cite{inproceedings}, demonstrating moderate success when combined with fuzzy clustering. Deformable models, including active contours (snakes) and level sets, introduce shape priors and regularization, allowing more flexible adaptation to object boundaries. Active contour models have been successfully applied in medical image segmentation, including brain scans, to delineate ischemic stroke lesions \cite{qianActiveContourModel2013}. Similarly, statistical and deformable model approaches have been explored specifically for MRI-based ischemic stroke segmentation, highlighting their potential and limitations in handling heterogeneous lesion shapes \cite{steinStatisticalDeformableModel2001}. Although these methods represented an advance over purely intensity-based techniques and provided valuable insights in early neuroimaging studies, they remain prone to convergence issues, require careful parameter tuning, and are rarely competitive against modern learning-based approaches.
%
%Classical machine learning methods brought the use of data-driven classifiers trained on handcrafted features. Typical features included intensity histograms, texture descriptors, local gradients, or wavelet coefficients. Support vector machines (SVMs), random forests, and shallow neural networks were widely applied. For example, Geremia et al. \cite{geremiaSpatialDecisionForests2011} demonstrated the potential of random forests for lesion segmentation in multiple sclerosis, showing improved generalization compared to thresholding. In the context of ischemic stroke, random forests and SVMs have been applied to diffusion-weighted and FLAIR MRI to segment infarcted regions, achieving moderate accuracy \cite{inproceedings}. These studies highlight that while classical machine learning approaches can improve segmentation over simple intensity-based methods, their performance is limited by the quality and representativeness of handcrafted features. Another limitation is that feature extraction pipelines are often dataset-specific, reducing transferability to new data. In addition, these models do not scale well to high-dimensional medical imaging datasets compared to modern deep learning approaches.
%
%Deep learning methods have redefined the state of the art in medical image segmentation. Convolutional neural networks (CNNs) automatically learn hierarchical representations of the data, reducing the need for manual feature engineering. Fully convolutional networks (FCNs) introduced end-to-end learning for dense predictions. The U-Net architecture \cite{ronnebergerUNetConvolutionalNetworks2015} became a landmark in biomedical image segmentation, with its encoder-decoder structure and skip connections enabling high-level contextual feature extraction and fine spatial localization, particularly relevant for small ischemic infarcts. Variants such as residual U-Nets, attention U-Nets \cite{oktayAttentionUNetLearning2018}, and multi-scale 3D U-Nets \cite{kamnitsasEfficientMultiscale3D2017} have demonstrated improved performance by incorporating residual connections, attention mechanisms, and multi-modal MRI inputs (e.g., DWI and FLAIR) for stroke lesion segmentation.
%
%Frameworks like nnU-Net \cite{isenseeAbstractNnUNetSelfadapting2019} propose a fully automated strategy, self-configuring network depth, patch size, preprocessing, and training parameters to achieve robust performance across multiple medical imaging tasks, including ischemic stroke. Beyond U-Net derivatives, ensembles of fully convolutional networks have been applied to stroke and white matter hyperintensity segmentation, showing enhanced robustness and accuracy \cite{liFullyConvolutionalNetwork2018}.
%
%Other deep learning architectures originally developed for volumetric or general segmentation have also been adapted to medical imaging. Examples include DeepMedic \cite{kamnitsasEfficientMultiscale3D2017} for multi-scale 3D CNNs with probabilistic inference, V-Net for volumetric segmentation, and 2D CNN-based frameworks for ischemic stroke lesion segmentation \cite{shah2DCNNBasedSegmentation2020,gheibiCNNResDeepLearning2023}. Additionally, general-purpose segmentation models such as SegNet and DeepLabv3+, initially designed for natural images, have been successfully applied to medical imaging tasks including stroke and brain lesion segmentation.
%
%More recently, YOLO-based models have been explored for ischemic stroke lesion detection and segmentation, leveraging their speed and efficiency for real-time clinical applications. For example, MSA-YOLOv5 \cite{chenMSAYOLOv5MultiscaleAttentionbased2023} incorporates multi-scale attention mechanisms to improve detection of small and low-contrast lesions in multimodal MRI images. Similarly, lightweight models such as TE-YOLOv5 \cite{chenAutomaticDetectionStroke2022} have demonstrated effective segmentation of diffusion-weighted imaging (DWI) stroke lesions while reducing computational requirements. These studies illustrate the potential of YOLO architectures to complement traditional 3D CNNs, offering rapid inference and promising accuracy, particularly in scenarios where real-time performance is critical.
%
%These deep learning methods outperform traditional and classical machine learning approaches, especially in handling high-dimensional data and complex lesion variability. Nevertheless, they require large annotated datasets, substantial computational resources, and careful design choices or automated configuration strategies to generalize effectively across patients and imaging modalities.
%
%\subsection{Knowledge Gaps and Our Contribution} \label{subsec:gaps}
%
%Several knowledge gaps remain in the current state of the art. First, there is a scarcity of large annotated ischemic stroke datasets, especially for FLAIR sequences, which limits the potential of deep learning models. Second, while U-Net and nnU-Net are widely studied, the applicability of emerging general-purpose segmentation models like YOLOv8 in the biomedical domain remains underexplored. Third, few systematic comparative studies have been conducted across models with fundamentally different design philosophies: U-Net as a domain-specific baseline, nnU-Net as a self-configuring medical framework, and YOLOv8 as a general-purpose real-time model.
%
%The present study addresses these gaps by conducting a comparative analysis of U-Net, nnU-Net, and YOLOv8 for ischemic stroke lesion segmentation using FLAIR MRI. Although trained on a relatively small dataset, the results indicate that these models can achieve competitive performance. This suggests that even with limited annotated data, reliable automated segmentation is feasible. Furthermore, findings may be extrapolated to larger datasets in the future. By evaluating YOLOv8 alongside established medical architectures, this work contributes new insights into the potential of real-time segmentation methods in clinical imaging workflows, offering a perspective on how efficiency and accuracy can be balanced for practical applications.

% Reordered and reintegrated Related Work (single block, original text remains commented above):
Medical image segmentation has progressed from classical intensity‑ and model‑based techniques to modern data‑driven methods that learn hierarchical features end‑to‑end. Early approaches such as thresholding, region growing, deformable models and clustering were useful but are limited in the presence of low contrast, anatomical variability and scanner artifacts that commonly affect brain FLAIR images. In FLAIR specifically, region‑growing and level‑set (active contour) methods were historically applied to delineate hyperintense lesions, but they relied on manual seeding and careful parameter tuning that limited automation and generalization~\cite{qianActiveContourModel2013,steinStatisticalDeformableModel2001}. Classical machine learning (SVMs, random forests) improved performance by leveraging handcrafted features (texture, intensity histograms, gradients), but these pipelines remain dependent on feature design and struggle to scale to volumetric, high‑dimensional data without extensive engineering~\cite{geremiaSpatialDecisionForests2011}. The advent of convolutional neural networks enabled end‑to‑end dense prediction and a leap in performance: U‑Net and its volumetric extensions provided robust baselines for medical segmentation, while architectural advances (residual blocks, attention gates, multiscale 3D designs) further improved localization of small and diffuse lesions~\cite{ronnebergerUNetConvolutionalNetworks2015,oktayAttentionUNetLearning2018,He2016,Milletari2016}.

Benchmark challenges and public datasets have driven reproducible progress but also expose key limitations: annotated ischemic stroke collections—especially FLAIR‑only datasets—are relatively small, cross‑site variability persists, and few studies compare fundamentally different paradigms under matched preprocessing and evaluation protocols. Notably, the BRATS challenge established a widely used benchmark for brain tumor segmentation and helped standardize metrics and evaluation practices across studies~\cite{Menze2015}. However, dataset challenges (e.g., BRATS and the ISLES series) have been central to progress in brain lesion segmentation, providing standardized data, metrics and leaderboards that enable reproducible comparison and expose the limitations of intensity‑based approaches~\cite{Menze2015}. These benchmarks highlighted early on that classical image processing techniques—thresholding, region growing, clustering, active contours and graph cuts—are often insufficient for heterogeneous clinical MRI due to low contrast, motion artefacts and scanner variability~\cite{qianActiveContourModel2013,steinStatisticalDeformableModel2001,boykovInteractiveGraphCuts2001}. 

Deep learning—particularly convolutional neural networks—enabled a step change by providing end‑to‑end dense prediction. Architectures such as U‑Net (and its 3D variants) became standard baselines; residual and attention extensions and multiscale 3D designs further improved localization and robustness. Frameworks like nnU‑Net have shown that automated configuration of preprocessing, patch size and training schedules often outperforms manual tuning across heterogeneous medical tasks. Ensemble and multiscale strategies (e.g., DeepMedic, V‑Net) remain powerful when spatial context and postprocessing priors are integrated. Recently, general‑purpose, real‑time oriented models (notably YOLO‑family adaptations) and attention‑augmented detectors have been adapted to stroke imaging to enable rapid triage and real‑time screening; these approaches trade some voxel‑level precision for inference speed and often require domain‑specific losses and 3D reassembly to produce clinically useful volumetric masks~\cite{chenAutomaticDetectionStroke2022,chenMSAYOLOv5MultiscaleAttentionbased2023,gheibiCNNResDeepLearning2023}. Hybrid strategies that combine 2D speed with 3D refinement appear promising for clinical pipelines that demand both quick turnaround and spatial accuracy. 

Beyond architecture design, algorithmic pipelines matured along two complementary axes. First, multiscale 3D networks and ensemble strategies (DeepMedic, V‑Net and their variants) integrated spatial context and postprocessing priors to improve lesion continuity and reduce false positives~\cite{kamnitsasEfficientMultiscale3D2017,Milletari2016,liFullyConvolutionalNetwork2018}. Second, meta‑frameworks such as nnU‑Net automated many engineering choices (preprocessing, patch size, normalization and training schedules), often matching or exceeding manually tuned systems across diverse datasets~\cite{Isensee2021,isenseeAbstractNnUNetSelfadapting2019}.

Practical challenges remain: cross‑site generalization, domain shift and limited annotated FLAIR‑only collections constrain model robustness. Work on domain adaptation and adversarial learning, and evaluations on multi‑center datasets, addresses these issues but add complexity to training and validation workflows~\cite{Kamnitsas2017DA,Karani2018}. Survey articles and methodological reviews synthesize these findings and emphasize the need for larger, well‑curated datasets, standardized metrics (Dice, IoU) and transparent reporting of detection versus segmentation performance and uncertainty estimates~\cite{Litjens2017,minaeeImageSegmentationUsing2021,liuReviewDeepLearningBasedMedical2021,TahaHanbury2015}.

Taken together, the literature supports a pragmatic strategy: select self‑configuring volumetric frameworks (e.g., nnU‑Net) when accuracy and robustness are priorities; consider lightweight 2D/real‑time variants for speed‑critical screening, and always accompany deployment with rigorous cross‑site validation, careful preprocessing (resampling, intensity normalization) and task‑aware evaluation~\cite{Isensee2021,liFullyConvolutionalNetwork2018,Menze2015}.


\section{Methodology} 
\label{sec:methodology}

This section describes the dataset, the shared preprocessing pipeline, the implemented models and training procedures, the evaluation protocol, and the reproducibility measures adopted. The presentation is compact yet sufficiently detailed for replication in a scientific context.

\subsection{Dataset} 
\label{subsec:dataset}

The experiments were conducted using the publicly available ISLES 2022~\cite{liewLargeCuratedOpensource2021} (Ischemic Stroke Lesion Segmentation) dataset, developed as part of the international MICCAI challenge. The dataset contains 250 multimodal MRI cases from patients diagnosed with ischemic stroke, each accompanied by manually annotated lesion masks created by expert radiologists. Every case includes several MRI sequences—Dif\-fusion-Weighted Imaging (DWI), Apparent Diffusion Coefficient (ADC), and Fluid-Attenua\-ted Inversion Recovery (FLAIR)—which capture complementary physiological information of the infarcted tissue.

For this study, only the FLAIR sequence was used as model input and the expert lesion masks supplied by the challenge were treated as ground truth. FLAIR images suppress the cerebrospinal fluid (CSF) signal, enhancing the visibility of hyperintense regions that correspond to ischemic edema or infarction. This property makes FLAIR particularly suitable for the segmentation of small lesions, especially in subacute phases. Each image and its corresponding segmentation were provided in NIfTI format, which preserves the spatial and volumetric information of 3D neuroimaging data. To ensure consistency between the MRI volumes and their corresponding segmentation masks, a general preprocessing step was applied prior to model-specific procedures. The segmentation masks provided in the ISLES 2022 dataset did not always match the spatial dimensions of their respective FLAIR images, which could hinder the application of automated segmentation models. Therefore, a spatial resampling process was carried out to align each mask with its corresponding MRI volume. This adjustment was performed using nearest-neighbor interpolation, which assigns to each new voxel the value of the closest voxel in the original mask, thus preserving the discrete nature of the labeled regions and preventing the generation of invalid intermediate values. This preprocessing step was essential to guarantee the spatial coherence between input images and ground-truth segmentations, enabling consistent training and evaluation across all models. After this alignment, each model applied its own specific preprocessing pipeline, which is described in the corresponding subsections below.

%\subsection{Experimental Environment} \label{subsec:environment}

%All experiments were conducted using Python-based frameworks for deep learning and medical image processing. Training was performed on a GPU-equipped environment (NVIDIA Tesla T4) to ensure efficient execution of 3D convolutional networks. The implementations relied on TensorFlow/Keras for U-Net, nnU-Net for the self-configuring model, and the Ultralytics framework for YOLOv8. The setup ensured full reproducibility across experiments.

\subsection{Model Architectures and Training Procedures}\label{subsec:models}

All experiments were conducted using Python-based frameworks for deep learning and medical image processing. Training was performed on a GPU-equipped environment (NVIDIA Tesla T4) to ensure efficient execution of 3D convolutional networks. The implementations relied on TensorFlow/Keras for U-Net~\cite{ronnebergerUNetConvolutionalNetworks2015,Zhou2018}, nnU-Net~\cite{isenseeAbstractNnUNetSelfadapting2019} for the self-configuring model, and the Ultralytics framework for YOLOv8~\cite{jocherUltralyticsYolov5V702022}. The setup ensured full reproducibility across experiments.
%
Therefore, three segmentation models were implemented and compared: U-Net, nnU-Net, and YOLOv8. These architectures were chosen to represent three complementary paradigms in medical image segmentation: a manually designed biomedical model, an automated self-configuring framework, and a general-purpose segmentation architecture adapted from object detection.
%
Each model was independently trained using the same dataset split, preprocessed FLAIR volumes, and evaluation metrics to ensure a fair comparison. Training configurations are summarized in Table~\ref{tab:model_comparison}.


% \begin{table}[tp]
% \centering
% \caption{Comparison of U-Net, nnU-Net, and YOLOv8 Parameters}
% \label{tab:model_comparison}
% \resizebox{\textwidth}{!}{%
% \begin{tabular}{llll}
% \toprule
% Parameter & U-Net & nnU-Net & YOLOv8 \\
% \midrule
% Input Type & 3D (192×192×64) & 3D full-resolution & 2D slices (reconstructed to 3D) \\
% Configuration Type & Manual & Automatic (self-configuring) & Default (Ultralytics) \\
% Optimizer & Adam (lr = 1e$^{-4}$) & Default SGD (automatic) & Default (automatic) \\
% Loss Function & BCE + Dice & Soft Dice + Cross Entropy (default) & Segmentation loss (default) \\
% Epochs & 100 (stopped at 24, early stopping) & 141 (manual stop) & 100 (stopped at 50, manual stop) \\
% Batch Size & 1 & Automatic (patch-based) & 8 \\
% Data Augmentation & Flips, rotations (±15°) & Automatic (rotations, elastic deformations, brightness/contrast) & Flips, rotations, contrast \\
% Evaluation Metrics & Dice & Dice & Precision, Recall, mAP; Dice (on test) \\
% \bottomrule
% \end{tabular}
% }
% \end{table}

\begin{table}[tp]
\centering
\caption{Comparison of U-Net, nnU-Net, and YOLOv8 Parameters}
\label{tab:model_comparison}
\begin{tabularx}{\textwidth}{@{} l l X @{}}
\toprule
Parameter & Model & Setting \\
\midrule
\multirow{3}{*}{Input Type} 
  & U-Net    & 3D (192×192×64) \\
  & nnU-Net  & 3D full-resolution \\
  & YOLOv8   & 2D slices (reconstructed to 3D) \\
\addlinespace
\multirow{3}{*}{Configuration Type} 
  & U-Net    & Manual \\
  & nnU-Net  & Automatic (self-configuring) \\
  & YOLOv8   & Default (Ultralytics) \\
\addlinespace
\multirow{3}{*}{Optimizer} 
  & U-Net    & Adam (lr = 1e$^{-4}$) \\
  & nnU-Net  & SGD + momentum (nnU‑Net default) \\
  & YOLOv8   & Ultralytics default optimizer \\
\addlinespace
\multirow{3}{*}{Loss Function} 
  & U-Net    & BCE + Dice (0.5/0.5) \\
  & nnU-Net  & Soft Dice + Cross‑Entropy (default) \\
  & YOLOv8   & Segmentation loss (framework default) \\
\addlinespace
\multirow{3}{*}{Epochs / Stopping} 
  & U-Net    & up to 100 (early stopping; stopped at 24) \\
  & nnU-Net  & default schedule (representative run: 141) \\
  & YOLOv8   & up to 100 (representative stop ~50) \\
\addlinespace
\multirow{3}{*}{Batch Size} 
  & U-Net    & 1 (GPU constrained) \\
  & nnU-Net  & patch‑based (automatic) \\
  & YOLOv8   & 8 (tuned to GPU memory) \\
\addlinespace
\multirow{3}{*}{Data Augmentation} 
  & U-Net    & flips, rotations (±15°), elastic, intensity jitter \\
  & nnU-Net  & extensive automatic augmentations (spatial/intensity) \\
  & YOLOv8   & flips, rotations, contrast/brightness adjustments \\
\addlinespace
\multirow{3}{*}{Evaluation Metrics} 
  & U-Net    & Dice, IoU \\
  & nnU-Net  & Dice, IoU \\
  & YOLOv8   & Dice, IoU, detection metrics (precision/recall/mAP) \\
\bottomrule
\end{tabularx}
\end{table}



%\subsubsection{U-Net} \label{subsubsec:unet_arch}
The U-Net~\cite{ronnebergerUNetConvolutionalNetworks2015} architecture is a widely adopted convolutional neural network designed specifically for biomedical image segmentation. It consists of a contracting path (encoder) that captures context and a symmetric expanding path (decoder) that enables precise localization. The model was implemented using TensorFlow/Keras, following the original design with modifications to accommodate 3D volumetric data. Specifically, the encoder progressively downsamples the input through convolutional and max-pooling layers to extract hierarchical features, while the decoder reconstructs the segmentation map through upsampling and skip connections that fuse spatial details from the encoder with the decoder’s learned representations. This structure enables precise localization, particularly for small and irregular ischemic lesions.

Before model training, all images underwent a standardized preprocessing pipeline to ensure consistency across the dataset. First, the volumes were spatially resampled to an isotropic resolution of ($1 \times 1 \times 1 $) mm or \SI{1}{\milli\meter\cubed} using linear interpolation for the MRI scans and nearest-neighbor interpolation for the segmentation masks to maintain their discrete labels. Then, each volume was intensity normalized using z-score normalization---that is, subtracting the mean and dividing by the standard deviation---to standardize voxel intensities and facilitate model convergence. To ensure uniform input dimensions, all volumes were cropped or zero-padded to a fixed size of $192 \times 192 \times 64$ voxels, centered around the brain region to preserve relevant anatomical information.
%
During training, data augmentation was applied to increase variability and mitigate overfitting. Random 3D flips along spatial axes and in-plane rotations of up to $\pm15$° were performed with equal probability, always applied consistently to both images and masks. The dataset was divided into training (80\%), validation (10\%), and test (10\%) subsets, ensuring that patient cases did not overlap across partitions.

The 3D U-Net was configured with convolutional blocks consisting of two $3 \times 3 \times 3$ convolutions followed by batch normalization and ReLU activation. The encoder comprised four levels with 64, 128, 256, and 512 filters, and the bottleneck used 1024 filters. The decoder mirrored this configuration with deconvolutional layers followed by concatenation and convolutional refinement. The final layer was a  $1 \times 1 \times 1 $ convolution with sigmoid activation, producing voxel-wise probabilities for binary segmentation.

Training was performed for up to 100 epochs with a batch size of 1 due to GPU memory constraints. The Adam optimizer was used with a learning rate of 0.0001. The loss function combined Binary Cross-Entropy (BCE) and Dice Loss with equal weights ($\alpha = 0.5$), balancing voxel-level accuracy and spatial overlap. The Dice coefficient was monitored as the main validation metric.
%
Regularization strategies included early stopping (patience = 20 epochs), model checkpointing---saving the best model based on validation Dice---and CSV logging for metric tracking. Data augmentation was applied on the fly during training to enhance robustness.

%\subsubsection{nnU-Net} \label{subsubsec:nnunet_arch}

The nnU-Net \cite{Isensee2021} (no-new U-Net) framework is a self-configuring framework that automatically adapts its architecture, preprocessing, and training protocols to the specific characteristics of the dataset. The model was implemented using the official nnU-Net repository, which provides a streamlined pipeline for medical image segmentation tasks. A nnU-Net   was configured in its 3D full-resolution mode, which processes volumetric data without downsampling. Unlike manually tuned architectures, the framework automatically determined the optimal patch size, normalization strategy, and training schedule based on the ISLES dataset characteristics.
%
For this study, the 3D full-resolution nnU-Net configuration was selected, which operates directly on volumetric data without downscaling. The preprocessing pipeline, model depth, and patch size were generated automatically by the framework. The network architecture retained the core principles of U-Net but adapted parameters such as kernel size, number of feature maps, and training schedule to the ISLES dataset.

Training was performed on a single fold of the default five-fold cross-validation scheme provided by nnU-Net, due to computational constraints. Although nnU-Net is designed to train and ensemble multiple folds for optimal generalization, this study focused on one representative fold to evaluate the feasibility and performance of the approach.
%
The training process followed the default nnU-Net configuration, which automatically applies a combination of Dice and Cross-Entropy loss and optimizes the network using stochastic gradient descent (SGD) with momentum, was employed. The initial learning rate was set to 0.01, with a polynomial decay schedule. The model was trained with a batch size of 2, constrained by GPU memory limitations.
%
During training, the framework also performed extensive on-the-fly data augmentation, including random spatial transformations (rotations, scaling, and elastic deformations) and intensity augmentations such as brightness and contrast adjustments. These operations were applied probabilistically to each patch, enhancing the model’s robustness to anatomical variability and scanner-related differences.
%
The model was trained for 141 epochs, at which point the training was manually stopped once convergence trends were observed. Despite the reduced training schedule, the results obtained were consistent with those reported in the literature, confirming the robustness of the nnU-Net framework even under limited computational resources.

%\subsubsection{YOLOv8} \label{subsubsec:yolov8_arch}

YOLOv8~\cite{jocherUltralyticsYolov5V702022} is one of the latest iterations of the YOLO (You Only Look Once) family of models, originally designed for real-time object detection. For this study, YOLOv8 was adapted for 2D medical image segmentation by modifying its architecture to output pixel-wise class probabilities instead of bounding boxes. The model was implemented using the Ultralytics YOLOv8 framework, which provides a flexible and efficient platform for training and deploying YOLO models.
%
The YOLOv8-seg model extends the well-known object detection architecture into instance segmentation. It combines real-time efficiency with pixel-level prediction by integrating a convolutional backbone, a neck for multi-scale feature fusion, and segmentation heads that output binary masks for detected regions. Although originally designed for natural images, its adaptability and speed make it an interesting candidate for biomedical applications. 

For adaptation to MRI data, the input images were converted into 2D slices extracted from the FLAIR volumes, maintaining spatial correspondence with their 3D masks. The model was initialized with random weights and trained specifically on the FLAIR images for binary ischemic lesion segmentation, without using any external pretrained parameters.
%
Training followed the default configuration provided by the Ultralytics YOLOv8-seg framework, which automatically defines optimization parameters and learning rate scheduling. The model was trained for 100 epochs with a batch size of 8, using the framework’s segmentation loss component as the primary objective.
%
During training, standard data augmentations were applied, including random flips, rotations, and contrast variations, to improve robustness and reduce overfitting.
%
While YOLOv8 processes data in 2D, the resulting slice-wise masks were reassembled into volumetric segmentations during postprocessing, enabling fair comparison with the 3D U-Net and nnU-Net outputs.

%\subsection{Evaluation Metrics} 
%\label{subsec:metrics}

Models performance was evaluated using the two most common overlap-based segmentation metrics: Dice Similarity Coefficient (DSC) and Intersection over Union (IoU). YoloV8 performance was monitored on the validation set using precision, recall, and mean Average Precision (mAP) metrics, as implemented by the YOLOv8 evaluation pipeline.
%
After training, the Dice coefficient was computed on the independent test set to provide a direct comparison with the U-Net and nnU-Net segmentation models.
The Dice coefficient quantifies the harmonic mean between precision and recall at the voxel level, providing a measure of overlap between predicted and reference masks. It is particularly suited for medical imaging tasks with imbalanced classes, such as small ischemic lesions. The IoU measures the ratio between the intersection and the union of predicted and ground-truth regions, offering a stricter assessment of segmentation accuracy.
%
Both metrics were computed on the test set for each model. Higher values indicate better segmentation performance, with 1 representing perfect overlap.

%\subsection{Ethical considerations} 
%\label{subsec:ethics}

%This study utilized publicly available, anonymized datasets, ensuring compliance with ethical standards for medical research. No new patient data were collected, and all analyses were conducted in accordance with relevant guidelines and regulations.



\section{Results}\label{sec:results}

This section presents the quantitative and qualitative results obtained from the three segmentation models: U-Net, nnU-Net, and YOLOv8. Performance metrics include Dice coefficient, Intersection over Union (IoU), and, in the case of YOLOv8, precision, recall, and mean Average Precision (mAP) was obtained too. Figures and tables illustrate representative outcomes and training behaviors.

%\subsection{U-Net Results}\label{subsec:unet}

The 3D U-Net training was automatically stopped after 24 epochs due to the early stopping criterion, as shown in Figure~\ref{fig:unet_loss_curve}, which depicts the training loss curve. The validation Dice coefficient increased steadily before reaching a stable value around epoch 20. Quantitative results for the test set are summarized in the first row of Table~\ref{tab:comparative_metrics} for the U-Net model.
%
Qualitative examples of the model outputs are presented in Figure~\ref{fig:unet_qualitative}, showing the ground truth in green and predicted segmentation masks in red, in both 2D slices and 3D reconstructions for representative test cases. Figure~\ref{fig:unet_qualitative} illustrates the best-performing case, with a Dice coefficient of 0.811, and an intermediate case, with a Dice of 0.342, highlighting the variability in segmentation accuracy across the dataset.
%
\begin{figure}[tp]
    \centering
    \includegraphics[width=\textwidth]{figures/Figure 1.jpg}
    \caption{Training and validation loss curves for the 3D U-Net model over 24 epochs. The training loss (blue) decreases steadily, while the validation Dice coefficient (orange) improves and stabilizes around epoch 20, indicating convergence.}\label{fig:unet_loss_curve}
\end{figure}
% \begin{table}[tp]
% \centering
% \begin{tabular}{lc}
% \toprule
% \textbf{Metric} & \textbf{Mean ± SD} \\
% \midrule
% Dice & $0.294 \pm 0.282$ \\
% IoU & $0.207 \pm 0.221$ \\
% \bottomrule
% \end{tabular}
% \caption{Performance Metrics: 3D U-Net on the Test Set (N=25). The model achieved a mean Dice coefficient of 0.294 and an IoU of 0.207, with considerable variability across cases, as indicated by the standard deviation.}\label{tab:unet_metrics}
% \end{table}
\begin{table}[tp]
\centering
\caption{Performance metrics (mean $\pm$ SD) on the test set (N=25) for the three compared models. The U-Net model achieved a mean Dice coefficient of 0.294 and an IoU of 0.207, with considerable variability across cases, as indicated by the standard deviation. The nnU-Net and YOLOv8 models demonstrated improved performance, with mean Dice coefficients of 0.483 and 0.476, respectively, and IoU values of 0.362 and 0.358. These results highlight the advantages of automated configuration and real-time segmentation approaches over the classical U-Net architecture in this ischemic stroke lesion segmentation task.}\label{tab:comparative_metrics}
\label{tab:comparative_metrics}
\begin{tabular}{lcc}
\toprule
Model & Dice & IoU \\
\midrule
3D U-Net       & $0.294 \pm 0.282$ & $0.207 \pm 0.221$ \\
nnU-Net        & $0.483 \pm 0.284$ & $0.362 \pm 0.244$ \\
YOLOv8-seg     & $0.476 \pm 0.287$ & $0.358 \pm 0.254$ \\
\bottomrule
\end{tabular}
\end{table}
%
\begin{figure}[tp]
    \centering
    \includegraphics[width=\textwidth]{figures/Figure 2.jpg}
    \caption{Qualitative Results: 3D U-Net Segmentations. The middle-left side shows the best-performing test case (Dice = 0.811), while the middle-right side presents an intermediate case (Dice = 0.342). Each case includes axial, sagittal, and coronal views of the FLAIR image with overlaid ground truth (green) and predicted segmentation (red), as well as a 3D rendering of the lesion. The model accurately captures lesion boundaries in the best case but struggles with diffuse regions in the intermediate case.}
    \label{fig:unet_qualitative}
\end{figure}

%\subsection{nnU-Net Results} \label{subsec:nnu-net}  

%The nnU-Net model was trained for 141 epochs, with training and validation loss curves shown in Figure~\ref{fig:nnu-net_loss_curve}. The validation Dice coefficient improved consistently, stabilizing around epoch 120. Test set results are summarized in Table~\ref{tab:comparative_metrics} in its second row.
%
The nnU-Net model automatically configured its preprocessing, patch size, and training schedule. It reached optimal validation performance after 141 epochs, as indicated by the decreasing loss curve and the rising pseudo-Dice score, shown in Figure~\ref{fig:nnu-net_loss_curve}. The average results on the test set are displayed in the second row of Table~\ref{tab:comparative_metrics} for the nnU-Net model.
%
Qualitative results demonstrated that nnU-Net effectively captured lesion boundaries and spatial continuity, even in cases with irregular shapes or heterogeneous intensities. Figure~\ref{fig:nnunet_qualitative} illustrates representative segmentations for two test cases (Dice 0.824 and 0.480), showing good agreement between the predicted and manual annotations.

\begin{figure}[tp]
    \centering
    \includegraphics[width=\textwidth]{figures/Figure 3.jpg}
    \caption{Training and validation loss curves for the nnU-Net model over 141 epochs. The training loss (blue) decreases steadily, while the validation Dice coefficient (orange) improves and stabilizes around epoch 120, indicating convergence.}\label{fig:nnu-net_loss_curve}
\end{figure}

% \begin{table}[tp]
%   \centering
%   \begin{tabular}{lc}
%   \toprule
%   Metric & Mean $\pm$ SD \\
%   \midrule
%   Dice & $0.483 \pm 0.284$ \\
%   IoU & $0.362 \pm 0.244$ \\
% \bottomrule
% \end{tabular}
% \caption{Performance Metrics: nnU-Net on the Test Set (N=25). The model achieved a mean Dice coefficient of 0.483 and an IoU of 0.362, demonstrating improved accuracy and robustness compared to the classical U-Net.}\label{tab:nnunet_metrics}
% \end{table}

\begin{figure}[tp]    
    \centering
    \includegraphics[width=\textwidth]{figures/Figure 4.jpg}
    \caption{Qualitative Results: nnU-Net Segmentations. The left side shows a high-performing test case (Dice = 0.824), while the right side presents a moderate case (Dice = 0.480). Each case includes axial, sagittal, and coronal views of the FLAIR image with overlaid ground truth (green) and predicted segmentation (red), as well as a 3D rendering of the lesion. The model accurately captures lesion boundaries in both cases, demonstrating its robustness across varying lesion characteristics.}
    \label{fig:nnunet_qualitative}
\end{figure}

%\subsection{YOLOv8 Results} \label{subsec:yolov8}

The YOLOv8-seg model was trained for 100 epochs, with training and validation loss curves shown in Figure~\ref{fig:yolov8_loss_curve}. The loss curves showed rapid convergence after 50 epochs. The model achieved a mean Average Precision (mAP) of 0.512 on the validation set. The YOLOv8 segmentation variant was trained on 2D FLAIR slices, which were reconstructed into 3D volumes for evaluation. Test set results, including Dice coefficient, IoU, precision, and recall, are summarized in the third row of Table~\ref{tab:comparative_metrics} and in Table~\ref{tab:yolov8_detection_metrics}.
%
\begin{figure}[tp]
    \centering
    \includegraphics[width=.7\textwidth]{figures/Figure 5.jpg}
    \caption{Training and validation loss curves for the YOLOv8-seg model over 100 epochs. The training loss (blue) decreases rapidly, while the validation Dice coefficient (orange) improves and stabilizes around epoch 50, indicating convergence.}\label{fig:yolov8_loss_curve}
\end{figure}
%
% \begin{table}[tp]
% \centering
% \begin{tabular}{lc}
% \toprule
% Metric & Mean $\pm$ SD \\
% \midrule
% Dice & $0.476 \pm 0.287$ \\
% IoU & $0.358 \pm 0.254$ \\
% \bottomrule
% \end{tabular}
% \caption{Performance Metrics: YOLOv8-seg on the Test Set (N=25). The model achieved a mean Dice coefficient of 0.476 and an IoU of 0.358, demonstrating competitive performance with efficient inference capabilities.}\label{tab:yolov8_metrics}
% \end{table}
%
\begin{table}[tp]
\centering
\begin{tabular}{lc}
\toprule
Metric & Mean $\pm$ SD \\
\midrule
Precision & $0.437 \pm 0.057$ \\
Recall & $0.213 \pm 0.031$ \\
mAP@0.5 & $0.202 \pm 0.034$ \\
mAP@0.5:0.95 & $0.073 \pm 0.014$ \\
\bottomrule
\end{tabular}
\caption{Performance Metrics: YOLOv8 on the Test Set (N=25). The model achieved a mean Average Precision (mAP) of 0.202 at IoU threshold 0.5 and 0.073 at IoU thresholds from 0.5 to 0.95, indicating its effectiveness in detecting ischemic lesions with reasonable precision and recall.}\label{tab:yolov8_detection_metrics}
\end{table}

\begin{figure}[tp]
    \centering
    \includegraphics[width=\textwidth]{figures/Figure 6.jpg}
    \caption{Qualitative Results: YOLOv8 Segmentations. The left side shows a high-performing test case (Dice = 0.792), while the right side presents a moderate case (Dice = 0.365). Each case includes axial, sagittal, and coronal views of the FLAIR image with overlaid ground truth (green) and predicted segmentation (red), as well as a 3D rendering of the lesion. The model captures major lesion areas but may miss finer details compared to 3D models.}\label{fig:yolov8_qualitative}
\end{figure}


%\subsection{Summarized Results Comparison} \label{subsec:comparison}

In summary, the three models demonstrated varying levels of performance in ischemic lesion segmentation from FLAIR MRI images. The U-Net model provided a baseline with moderate accuracy but exhibited significant variability across cases. The nnU-Net outperformed the classical U-Net by leveraging automated configuration, achieving higher Dice and IoU scores with improved robustness. YOLOv8, while originally designed for object detection, delivered competitive segmentation results with the added benefit of rapid inference times, making it suitable for real-time applications. A comparative overview of the three architectures is provided in Table~\ref{tab:comparison}. In this table is shown that the nnU-Net achieved the highest Dice and IoU values across all patient cases, followed by YOLOv8 and U-Net. These quantitative findings confirm the influence of model design and dimensionality on segmentation accuracy.

\begin{table}[tp]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lccc}
\toprule
Model & Dice & IoU & Notes \\
\midrule
U-Net & $0.294 \pm 0.282$ & $0.207 \pm 0.221$ & Stable training; limited generalization to complex lesions \\
nnU-Net & $0.483 \pm 0.284$ & $0.362 \pm 0.244$ & Best overall performance and most consistent segmentation \\
YOLOv8 & $0.476 \pm 0.287$ & $0.358 \pm 0.254$ & Fastest inference; lower boundary accuracy \\
\bottomrule
\end{tabular}
}
\caption{Comparison of segmentation models on the test set (N=25). The nnU-Net achieved the highest Dice and IoU scores, indicating superior segmentation accuracy and robustness. YOLOv8 provided competitive performance with efficient inference, while U-Net showed limited generalization capabilities.}
\label{tab:comparison}
\end{table}

\section{Discussion}\label{sec:discussion}

%This section discusses the comparative performance of the three segmentation models, highlighting their strengths and limitations, as well as potential clinical implications and future research directions.

The study was conducted entirely using publicly available and anonymized data, thus requiring no additional ethical approval. The ISLES 2022 dataset complies with the ethical and legal standards of medical data sharing and was collected following informed consent procedures by its original providers.
%
No personally identifiable information was processed, and all experiments were performed in compliance with the European General Data Protection Regulation (EGDPR) for research data handling. The study did not involve any clinical interventions or new patient data acquisition.

The comparative analysis highlights distinct characteristics in the performance of U-Net, nnU-Net, and YOLOv8 for ischemic lesion segmentation in FLAIR MRI images.
%
The U-Net architecture achieved moderate performance across the validation and test sets. Despite its well-established effectiveness in biomedical image segmentation, the model exhibited considerable variability among individual cases. Quantitatively, the Dice and IoU metrics indicated that the network was capable of correctly identifying lesions in certain cases but failed to generalize consistently across the dataset. This uneven behavior suggests that the classical U-Net is highly sensitive to image quality and lesion contrast. In cases where boundaries were well defined, segmentation was accurate, while diffuse or low-contrast lesions often led to missed detections.
%
The relatively high standard deviation observed in the test results indicates limited robustness. This variability may stem from the restricted size of the dataset and from the intrinsic heterogeneity of ischemic lesions. Additionally, the classic U-Net requires extensive manual configuration, including input size definition, normalization strategy, and data augmentation parameters. These manual design choices can introduce performance fluctuations depending on how well they adapt to the dataset characteristics.

The nnU-Net outperformed the classical U-Net in both validation and test phases. Its self-configuring mechanism automatically selected preprocessing, normalization, and training hyperparameters, leading to higher Dice and IoU scores. The model demonstrated a strong ability to adapt to the dataset and segment complex or irregular lesions, confirming the effectiveness of automated optimization. However, some test cases still produced null segmentations, revealing that even advanced configurations can struggle with highly heterogeneous or low-quality images.
%
The improved consistency observed in nnU-Net reflects its capacity to generalize better from limited data. Nevertheless, the standard deviation across test results indicates residual instability, possibly due to the reduced number of samples and to contrast variations between scans. In addition, nnU-Net is computationally demanding, requiring long training times and significant GPU resources, which may limit its practical deployment in resource-constrained environments.

Although originally designed for object detection rather than medical segmentation, YOLOv8 achieved competitive results in this study. Its inference speed and implementation simplicity stand out as major advantages, making it suitable for real-time applications. The model achieved respectable Dice and IoU values, occasionally matching those of nnU-Net in favorable cases. However, it also exhibited variability, with some test cases showing incomplete or missing lesion predictions.
%
This behavior can be explained by YOLOv8’s design, which prioritizes detection speed and bounding-box optimization over pixel-level precision. Consequently, the model performs well when lesions are clearly delimited but tends to underestimate diffuse or low-contrast regions. Despite these limitations, YOLOv8 demonstrates strong potential for fast and flexible segmentation pipelines, especially if adapted with domain-specific augmentations or loss functions.

When comparing all three architectures, nnU-Net achieved the highest accuracy and stability, confirming the benefits of automatic configuration in medical image segmentation. U-Net, while less resource-intensive, showed limited generalization, particularly in challenging cases. YOLOv8, though less precise, offered the best balance between speed and usability. These results emphasize that performance depends not only on network complexity but also on the suitability of the architecture to the dataset characteristics. 

However, several limitations must be acknowledged. The study was conducted on a relatively small dataset of 250 cases, using only FLAIR sequences, which restricts the models’ ability to exploit complementary multimodal information (e.g., DWI or ADC). Computational constraints limited full cross-validation in nnU-Net and reduced hyperparameter optimization in the other models. Moreover, the evaluation focused exclusively on segmentation accuracy without assessing the real clinical impact.

Future research could focus on incorporating multimodal MRI sequences (e.g., DWI and ADC) to improve lesion characterization and model generalization. In addition to classical data augmentation techniques, the creation of larger and more diverse datasets through the inclusion of new clinical cases and the integration of complementary public databases would be essential to achieve greater robustness. From a methodological perspective, combining 3D segmentation architectures with lightweight 2D frameworks like YOLOv8 could lead to hybrid models that preserve spatial precision while maintaining high computational efficiency. Such developments would contribute to the design of reliable and time-efficient automated systems for early ischemic stroke screening and diagnosis.

\section{Conclusion}\label{sec:conclusion}

%This study presented a comparative analysis of three deep learning architectures—U-Net, nnU-Net, and YOLOv8—for the segmentation of ischemic stroke lesions in FLAIR MRI images. Each model demonstrated unique strengths and limitations, influenced by their design philosophies and adaptability to the dataset characteristics.

This study presented a comparative analysis of three deep learning architectures—U-Net, nnU-Net, and YOLOv8—for the automatic segmentation of ischemic stroke lesions in FLAIR MRI images. The main objective was to evaluate their performance, analyze their respective strengths and weaknesses, and assess their feasibility for clinical use. The results demonstrate that deep learning techniques can achieve accurate and reproducible segmentation of ischemic lesions even without the use of contrast-enhanced imaging, supporting the potential of artificial intelligence for early stroke detection and screening.
%
The experimental findings show that nnU-Net achieved the highest overall accuracy, obtaining the best Dice and IoU scores and exhibiting strong generalization across cases. Its self-configuring design proved particularly effective in adapting the model to the dataset characteristics, minimizing the need for manual tuning. U-Net, despite its widespread use as a baseline in biomedical segmentation, achieved the lowest performance among the evaluated models. Its results were less accurate and more variable, confirming the limitations of manually configured architectures when applied to heterogeneous clinical data. YOLOv8, originally designed for object detection, offered competitive performance and the fastest inference time, highlighting its potential for real-time or resource-constrained applications.
%
These outcomes indicate that no single architecture dominates in all aspects; rather, each offers complementary advantages. nnU-Net is best suited for research and diagnostic contexts requiring high accuracy, U-Net serves as a methodological baseline that provides a reference for evaluating more advanced models, and YOLOv8 enables rapid deployment in clinical workflows where processing speed is crucial. Together, they demonstrate that automated segmentation of ischemic lesions is technically feasible and can be integrated into radiological pipelines to support decision-making and reduce manual workload.
%
The achievement of accurate segmentation without contrast administration has significant implications for non-invasive and cost-efficient stroke assessment. Automated detection can assist radiologists in identifying subtle ischemic regions that might otherwise be overlooked, standardize the diagnostic process, and accelerate treatment planning. These models demonstrate strong potential for real-world implementation as assistive tools within hospital imaging systems or screening platforms. With further validation on larger and more diverse datasets, they could evolve into clinically deployable AI solutions capable of supporting early diagnosis, personalized monitoring, and potentially near real-time MRI analysis in emergency settings.

In conclusion, this work confirms that deep learning-based segmentation provides a promising pathway toward automated, precise, and accessible tools for ischemic stroke detection. By combining accuracy, adaptability, and computational efficiency, the explored architectures contribute to advancing medical imaging toward faster and more reliable diagnostic support. The integration of such AI-driven solutions into clinical practice could significantly enhance patient care, enabling earlier intervention and better outcomes in one of the world’s leading causes of disability and mortality.

\printbibliography[heading=bibintoc,title={References}]

% haz una seccion de agradecimientos si es necesario
\section*{Acknowledgments}
The authors would like to thank the providers of the ISLES 2022 dataset for making this valuable resource publicly available, which greatly facilitated this research. We also acknowledge the support of our institution's computational resources that enabled the training and evaluation of the deep learning models presented in this study.
\end{document}
